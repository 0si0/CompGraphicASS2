import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torch.optim.lr_scheduler import LinearLR
import torch.nn as nn
import torch
import gc
import os
import imageio
import yaml
from torch.optim import Adam
from datetime import datetime
from PIL import Image
import requests
import nbconvert
from google.colab import drive

# File upload (Make sure 'Image-01.png' is uploaded to /content/)
image_file_name = "Image-01.png"
image_path = os.path.join("/content/", image_file_name) # Basic upload path

# Open image
try:
    original_image = Image.open(image_path)
    print(f"'{image_file_name}' Successfully opened.")
except FileNotFoundError:
    print(f"Cannot find image. Please upload '{image_file_name}' to /content/")
except Image.UnidentifiedImageError:
    print(f"{image_path} is not a valid image file.")
except Exception as e:
    print(f"Undefined error: {e}")

# [CODE BLOCK 1: Function Definition]

# [CODE BLOCK 1: Function Definition]

def generate_2D_gaussian_splatting(
    kernel_size, scale, rotation, coords, colours, image_size=(256, 256, 3),
    device="cpu", filter_type=None, chunk_size = 32, debug_print=False, track_grad=True):
    """
    Generates a 2D image by splatting multiple Gaussians.

    Args:
        kernel_size (int): Size of the square kernel for each Gaussian.
        scale (torch.Tensor): Tensor of shape (N, 2) representing the x and y scales of N Gaussians.
        rotation (torch.Tensor): Tensor of shape (N) representing the rotation angle (radians) of N Gaussians.
        coords (torch.Tensor): Tensor of shape (N, 2) representing the normalized ([-1, 1]) center coordinates (x, y) of N Gaussians.
        colours (torch.Tensor): Tensor of shape (N, 3) representing the RGB (with alpha pre-multiplied) colors of N Gaussians.
        image_size (tuple, optional): Target image dimensions (height, width, channels). Defaults to (256, 256, 3).
        device (str, optional): Device to run computations on ('cpu' or 'cuda'). Defaults to "cpu".
        filter_type (str, optional): Type of anti-aliasing filter.
                                     Can be 'low_pass', 'ewa', or None. Defaults to None.
        debug_print (bool, optional): If True, prints covariance matrix changes for debugging.
                                      Defaults to False.
    """

    batch_size = colours.shape[0]

    if not track_grad:
        scale = scale.detach()
        rotation = rotation.detach()
        coords = coords.detach()
        colours = colours.detach()

    # Ensure scale and rotation have the correct shape
    scale = scale.view(batch_size, 2)
    rotation = rotation.view(batch_size)

    # Compute the components of the covariance matrix
    cos_rot = torch.cos(rotation)
    sin_rot = torch.sin(rotation)

    R = torch.stack([
        torch.stack([cos_rot, -sin_rot], dim=-1),
        torch.stack([sin_rot, cos_rot], dim=-1)
    ], dim=-2)

    S = torch.diag_embed(scale)

    # Calculate the original covariance matrix: Σ_splat = R @ S @ S @ R^T
    covariance = R @ S @ S @ R.transpose(-1, -2)

    # For debugging: store the original covariance before filtering
    if debug_print:
        covariance_before = covariance.clone()

    # --- Apply anti-aliasing filters ---


    domain_radius = 5.0  # xx, yy = linspace(-5,5)
    pixel_size = (2*domain_radius) / (kernel_size - 1)  # 한 픽셀 너비
    pixel_radius = pixel_size / 2.0

    if filter_type == 'low_pass':
        # 최소 고유값이 pixel_variance 아래로 내려가지 않도록 보정
        a = covariance[:, 0, 0]
        b = covariance[:, 0, 1]
        c = covariance[:, 1, 1]
        trace = a + c
        det   = a * c - b * b
        # 음수 방지
        discr = torch.sqrt(torch.clamp(trace * trace - 4 * det, min=0.0))
        min_eig = 0.5 * (trace - discr)                # (N,)
        # 부족한 만큼만 아이덴티티 성분 추가
        blur_amount = torch.clamp(pixel_var_t - min_eig, min=0.0)  # (N,)
        blur_amount = blur_amount.view(-1, 1, 1)                  # (N,1,1)
        I = torch.eye(2, device=device, dtype=covariance.dtype).unsqueeze(0)  # (1,2,2)
        covariance = covariance + blur_amount * I                # (N,2,2)

    elif filter_type == 'ewa':
        # 픽셀 자체의 필터 분산을 모든 방향에 균등하게 추가
        pvar_local = (pixel_variance / (scale ** 2))        # (N,2)
        P = torch.diag_embed(pvar_local) 
        Σ_local = S @ S + P                                  # (N,2,2)
        covariance = R @ Σ_local @ R.transpose(-1, -2)


    # For debugging: print comparison of covariance matrices and eigenvalues
    if debug_print:
        scales_det = scale[:, 0] * scale[:, 1]
        target_idx = torch.argmin(scales_det)

        print(f"--- Debugging Anti-Aliasing Filter: '{filter_type if filter_type else 'None'}' ---\n")
        print(f"Target Gaussian Index (smallest by scale): {target_idx.item()}")
        print("Covariance Before Filtering (Smallest Gaussian):\n", covariance_before[target_idx].detach().cpu().numpy())
        print("Covariance After Filtering (Smallest Gaussian):\n", covariance[target_idx].detach().cpu().numpy())

        eig_before = torch.linalg.eigvalsh(covariance_before[target_idx])
        eig_after = torch.linalg.eigvalsh(covariance[target_idx])
        print("Eigenvalues Before Filtering:\n", eig_before.detach().cpu().numpy())
        print("Eigenvalues After Filtering:\n", eig_after.detach().cpu().numpy())
        print("-" * 60 + "\n")

    # Compute inverse covariance (will use the potentially filtered covariance)
    inv_covariance = torch.inverse(covariance)              # (N,2,2)
    det_vals = torch.det(covariance)                        # (N,)
    sqrt_det = torch.sqrt(det_vals)                         # (N,)

    # Prepare base grid for a single kernel
    H, W = image_size[0], image_size[1]
    x = torch.linspace(-5, 5, kernel_size, device=device)
    y = torch.linspace(-5, 5, kernel_size, device=device)
    xx, yy = torch.meshgrid(x, y, indexing='ij')
    base_xy = torch.stack([xx, yy], dim=-1)  # (K, K, 2)

    final_image = torch.zeros(3, H, W, device=device)

    for start in range(0, batch_size, chunk_size):
        end = min(start + chunk_size, batch_size)
        b = end - start

        inv_cov_chunk = inv_covariance[start:end]      # (b,2,2)
        coords_chunk = coords[start:end]               # (b,2)
        colours_chunk = colours[start:end]             # (b,3)
        sqrt_det_chunk = sqrt_det[start:end].view(b, 1, 1)

        # expand base_xy
        xy = base_xy.unsqueeze(0).expand(b, kernel_size, kernel_size, 2)  # (b,K,K,2)

        # Compute Gaussian kernel for this chunk
        z = torch.einsum('bxyi,bij,bxyj->bxy', xy, -0.5 * inv_cov_chunk, xy)  # (b,K,K)
        kernel = torch.exp(z) / (2 * torch.tensor(np.pi, device=device) * sqrt_det_chunk)  # (b,K,K)

        # apply colours
        kernel_rgb = kernel.unsqueeze(1).expand(-1, 3, -1, -1)  # (b,3,K,K)

        # padding
        pad_h = H - kernel_size
        pad_w = W - kernel_size
        padding = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)
        kern_pad = F.pad(kernel_rgb, padding, "constant", 0)    # (b,3,H,W)

        # translate
        theta = torch.zeros(b, 2, 3, device=device)
        theta[:, 0, 0] = 1.0
        theta[:, 1, 1] = 1.0
        theta[:, :, 2] = coords_chunk
        grid = F.affine_grid(theta, size=(b, 3, H, W), align_corners=True)
        splat = F.grid_sample(kern_pad, grid, align_corners=True)  # (b,3,H,W)

        # accumulate
        final_image += (colours_chunk.unsqueeze(-1).unsqueeze(-1) * splat).sum(dim=0)

        # free chunk memory
        del inv_cov_chunk, coords_chunk, colours_chunk, sqrt_det_chunk
        del xy, z, kernel, kernel_rgb, kern_pad, theta, grid, splat
        torch.cuda.empty_cache()

    # finalize
    final_image = torch.clamp(final_image, 0, 1)  # (3,H,W)
    return final_image.permute(1, 2, 0)


def create_window(window_size, channel):
    def gaussian(window_size, sigma):
        gauss = torch.exp(torch.tensor([-(x - window_size//2)**2/float(2*sigma**2) for x in range(window_size)]))
        return gauss/gauss.sum()

    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = torch.autograd.Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())

    return window



def ssim(img1, img2, window_size=11, size_average=True):

    (_, _, channel) = img1.size()

    img1 = img1.unsqueeze(0).permute(0, 3, 1, 2)
    img2 = img2.unsqueeze(0).permute(0, 3, 1, 2)


    # Parameters for SSIM
    C1 = 0.01**2
    C2 = 0.03**2

    window = create_window(window_size, channel)

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)

    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)
    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2

    SSIM_numerator = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))
    SSIM_denominator = ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))
    SSIM = SSIM_numerator / SSIM_denominator

    return torch.clamp((1 - SSIM) / 2, 0, 1)

def d_ssim_loss(img1, img2, window_size=11, size_average=True):
    return ssim(img1, img2, window_size, size_average).mean()

# Combined Loss
def combined_loss(pred, target, lambda_param=0.5):
    l1loss = nn.L1Loss()
    return (1 - lambda_param) * l1loss(pred, target) + lambda_param * d_ssim_loss(pred, target)



# Read the config.yml file
with open('config.yml', 'r') as config_file:
    config = yaml.safe_load(config_file)

# Extract values from the loaded config
KERNEL_SIZE = config["KERNEL_SIZE"]
image_size = tuple(config["image_size"])
primary_samples = config["primary_samples"]
backup_samples = config["backup_samples"]
num_epochs = config["num_epochs"]
densification_interval = config["densification_interval"]
learning_rate = config["learning_rate"]
image_file_name = config["image_file_name"]
display_interval = config["display_interval"]
grad_threshold = config["gradient_threshold"]
gauss_threshold = config["gaussian_threshold"]
display_loss = config["display_loss"]

def give_required_data(input_coords, image_size):

  # normalising pixel coordinates [-1,1]
  coords = torch.tensor(input_coords / [image_size[0],image_size[1]], device=device).float()
  center_coords_normalized = torch.tensor([0.5, 0.5], device=device).float()
  coords = (center_coords_normalized - coords) * 2.0

  # Fetching the colour of the pixels in each coordinates
  colour_values = [image_array[coord[1], coord[0]] for coord in input_coords]
  colour_values_np = np.array(colour_values)
  colour_values_tensor =  torch.tensor(colour_values_np, device=device).float()

  return colour_values_tensor, coords

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_samples = primary_samples + backup_samples

PADDING = KERNEL_SIZE // 2
image_path = image_file_name
original_image = Image.open(image_path)
original_image = original_image.resize((image_size[0],image_size[0]))
original_image = original_image.convert('RGB')
original_array = np.array(original_image)
original_array = original_array / 255.0
width, height, _ = original_array.shape

image_array = original_array
target_tensor = torch.tensor(image_array, dtype=torch.float32, device=device)
coords = np.random.randint(0, [width, height], size=(num_samples, 2))
random_pixel_means = torch.tensor(coords, device=device)
pixels = [image_array[coord[1], coord[0]] for coord in coords]
pixels_np = np.array(pixels)
random_pixels =  torch.tensor(pixels_np, device=device)

colour_values, pixel_coords = give_required_data(coords, image_size)

colour_values = torch.logit(colour_values)
pixel_coords = torch.atanh(pixel_coords)

scale_values = torch.logit(torch.rand(num_samples, 2, device=device))
rotation_values = torch.atanh(2 * torch.rand(num_samples, 1, device=device) - 1)
alpha_values = torch.logit(torch.rand(num_samples, 1, device=device))
W_values = torch.cat([scale_values, rotation_values, alpha_values, colour_values, pixel_coords], dim=1)


starting_size = primary_samples
left_over_size = backup_samples
persistent_mask = torch.cat([torch.ones(starting_size, dtype=bool),torch.zeros(left_over_size, dtype=bool)], dim=0)
current_marker = starting_size


# Get current date and time as string
now = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")

# Create a directory with the current date and time as its name
directory = f"{now}"
os.makedirs(directory, exist_ok=True)

W = nn.Parameter(W_values)
optimizer = Adam([W], lr=learning_rate)
scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=1e-8, total_iters=num_epochs)
loss_history = []


# [CODE BLOCK for Training Loop]

for epoch in range(num_epochs):

    # Find indices to remove and update the persistent mask (Pruning logic)
    if epoch % (densification_interval + 1) == 0 and epoch > 0:
        indices_to_remove = (torch.sigmoid(W[:, 3]) < 0.01).nonzero(as_tuple=True)[0]

        if len(indices_to_remove) > 0:
          print(f"Number of pruned points: {len(indices_to_remove)}")

        persistent_mask[indices_to_remove] = False

        # Zero-out parameters and their gradients for pruned Gaussians
        W.data[~persistent_mask] = 0.0


    gc.collect()
    torch.cuda.empty_cache()

    # Select active Gaussians based on persistent mask
    output = W[persistent_mask]

    batch_size = output.shape[0]

    # Decode Gaussian parameters from the output tensor
    scale = torch.sigmoid(output[:, 0:2])
    rotation = np.pi / 2 * torch.tanh(output[:, 2]) # Scale rotation to [-pi/2, pi/2]
    alpha = torch.sigmoid(output[:, 3])
    colours = torch.sigmoid(output[:, 4:7])
    pixel_coords = torch.tanh(output[:, 7:9]) # Coords are in [-1, 1] range

    colours_with_alpha  = colours * alpha.view(batch_size, 1)

    # --- Main Training Loop Rendering Call ---
    # The model is trained based on a BASELINE (no filter) output.
    # debug_print is OFF by default here for performance.
    g_tensor_batch = generate_2D_gaussian_splatting(
        KERNEL_SIZE, scale, rotation, pixel_coords, colours_with_alpha,
        image_size, device=device,
        filter_type=None, # <--- Model is trained with NO FILTER (Baseline)
        debug_print=False  # Keep debug_print off during main training
    )
    # Calculate loss against the target image
    loss = combined_loss(g_tensor_batch, target_tensor, lambda_param=0.2)

    optimizer.zero_grad() # Clear gradients before backpropagation
    loss.backward()       # Perform backpropagation to compute gradients

    # Apply zeroing out of gradients for pruned Gaussians
    if persistent_mask is not None:
        W.grad.data[~persistent_mask] = 0.0

    # Densification logic (Splitting and Cloning)
    if epoch % densification_interval == 0 and epoch > 0:

      # Calculate the norm of gradients
      gradient_norms = torch.norm(W.grad[persistent_mask][:, 7:9], dim=1, p=2) # Positional gradients
      gaussian_norms = torch.norm(torch.sigmoid(W.data[persistent_mask][:, 0:2]), dim=1, p=2) # Scale norms

      sorted_grads, sorted_grads_indices = torch.sort(gradient_norms, descending=True)
      sorted_gauss, sorted_gauss_indices = torch.sort(gaussian_norms, descending=True)

      large_gradient_mask = (sorted_grads > grad_threshold)
      large_gradient_indices = sorted_grads_indices[large_gradient_mask]

      large_gauss_mask = (sorted_gauss > gauss_threshold)
      large_gauss_indices = sorted_gauss_indices[large_gauss_mask]

      common_indices_mask = torch.isin(large_gradient_indices, large_gauss_indices)
      common_indices = large_gradient_indices[common_indices_mask]
      distinct_indices = large_gradient_indices[~common_indices_mask] # Those with large gradient but small scale

      # Split points with large coordinate gradient and large gaussian values and descale their gaussian
      if len(common_indices) > 0:
        print(f"Number of splitted points: {len(common_indices)}")
        start_index = current_marker + 1
        end_index = current_marker + 1 + len(common_indices)
        persistent_mask[start_index: end_index] = True
        W.data[start_index:end_index, :] = W.data[common_indices, :]
        scale_reduction_factor = 1.6
        W.data[start_index:end_index, 0:2] /= scale_reduction_factor
        W.data[common_indices, 0:2] /= scale_reduction_factor
        current_marker = current_marker + len(common_indices)

      # Clone points with large coordinate gradient but small gaussian values
      if len(distinct_indices) > 0:

        print(f"Number of cloned points: {len(distinct_indices)}")
        start_index = current_marker + 1
        end_index = current_marker + 1 + len(distinct_indices)
        persistent_mask[start_index: end_index] = True
        W.data[start_index:end_index, :] = W.data[distinct_indices, :]

        # Calculate the movement direction based on the positional gradient
        positional_gradients = W.grad[distinct_indices, 7:9]
        gradient_magnitudes = torch.norm(positional_gradients, dim=1, keepdim=True)
        normalized_gradients = positional_gradients / (gradient_magnitudes + 1e-8)

        step_size = 0.01

        # Move the cloned Gaussians slightly in gradient direction
        W.data[start_index:end_index, 7:9] += step_size * normalized_gradients

        current_marker = current_marker + len(distinct_indices)

    optimizer.step()
    scheduler.step()

    loss_history.append(loss.item())

    # --- Original Visualization Step ---
    # This visualization shows the output of the 'g_tensor_batch' (trained model output)
    # and the Ground Truth. It does not perform additional filter comparisons here.
    if epoch % display_interval == 0:
        num_subplots = 3 if display_loss else 2 # Default to 2 if loss is not displayed
        fig_size_width = 18 if display_loss else 12 # Default to 12 if loss is not displayed

        # Create subplots for generated image, ground truth, and optionally loss
        fig, ax = plt.subplots(1, num_subplots, figsize=(fig_size_width, 6))

        # Convert generated tensor to numpy for plotting
        generated_array = g_tensor_batch.cpu().detach().numpy()

        # Plot generated image (main output of the training loop)
        ax[0].imshow(generated_array)
        ax[0].set_title('2D Gaussian Splatting (Trained Baseline)') # Updated title
        ax[0].axis('off')

        # Plot Ground Truth image
        ax[1].imshow(target_tensor.cpu().numpy())
        ax[1].set_title('Ground Truth')
        ax[1].axis('off')

        # Optionally plot Loss vs Epochs
        if display_loss:
          ax[2].plot(range(epoch + 1), loss_history[:epoch + 1])
          ax[2].set_title('Loss vs. Epochs (Trained with Baseline)') # Updated title
          ax[2].set_xlabel('Epoch')
          ax[2].set_ylabel('Loss')
          ax[2].set_xlim(0, num_epochs)

        # Adjust layout and show plot
        plt.subplots_adjust(wspace=0.1)
        plt.pause(0.1) # Short pause to update plot in Colab

        # Save the generated image
        img = Image.fromarray((generated_array * 255).astype(np.uint8))
        filename = f"{epoch}.jpg"
        file_path = os.path.join(directory, filename)
        img.save(file_path)
        # fig.savefig(file_path, bbox_inches='tight') # Uncomment to save the matplotlib figure directly

        # Clear and close plot to free up memory
        plt.clf()
        plt.close()

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, on {len(output)} points")


  # [Filter Comparison After Training]

import matplotlib.pyplot as plt
import torch
import numpy as np # Ensure numpy is imported if not already global

print("--- Generating Comparison Renders from the Trained Model ---")

# Ensure global variables from previous cells are accessible:
# KERNEL_SIZE, image_size, device
# W (the trained Gaussian parameters after the main training loop)
# persistent_mask (to select active Gaussians from W)
# target_tensor (the original ground truth image)
# generate_2D_gaussian_splatting function (defined in a previous cell)

# 1. Extract final active Gaussian parameters from the trained model (W)
final_output = W[persistent_mask]

final_batch_size = final_output.shape[0]

# Decode parameters using the same logic as in the training loop
final_scale = torch.sigmoid(final_output[:, 0:2])
final_rotation = np.pi / 2 * torch.tanh(final_output[:, 2])
final_alpha = torch.sigmoid(final_output[:, 3])
final_colours = torch.sigmoid(final_output[:, 4:7])
final_pixel_coords = torch.tanh(final_output[:, 7:9])

final_colours_with_alpha = final_colours * final_alpha.view(final_batch_size, 1)

# 2. Render images with different filter types using the FINAL trained parameters
#    debug_print=True is enabled to show numerical changes in covariance for the smallest Gaussian.

# A. Baseline Render (No Filter)
print("Rendering Baseline (No Filter) from trained model...")
compare_baseline_render = generate_2D_gaussian_splatting(
    KERNEL_SIZE, final_scale, final_rotation, final_pixel_coords, final_colours_with_alpha,
    image_size, device=device,
    filter_type=None,       # Apply no filter
    debug_print=True        # Enable debug prints for this comparison
)

# B. Low-Pass Filter Render
print("Rendering with Low-Pass Filter from trained model...")
compare_low_pass_render = generate_2D_gaussian_splatting(
    KERNEL_SIZE, final_scale, final_rotation, final_pixel_coords, final_colours_with_alpha,
    image_size, device=device,
    filter_type='low_pass', # Apply Low-Pass filter
    debug_print=True
)

# C. EWA Filter Render
print("Rendering with EWA Filter from trained model...")
compare_ewa_render = generate_2D_gaussian_splatting(
    KERNEL_SIZE, final_scale, final_rotation, final_pixel_coords, final_colours_with_alpha,
    image_size, device=device,
    filter_type='ewa',      # Apply EWA filter
    debug_print=True
)

# 3. Visualize the comparison
fig_compare, axes_compare = plt.subplots(1, 4, figsize=(20, 6))

# Plot Baseline Render
axes_compare[0].imshow(compare_baseline_render.detach().cpu().numpy())
axes_compare[0].set_title('Final Render:\nBaseline (Trained Model)')
axes_compare[0].axis('off')

# Plot Low-Pass Filtered Render
axes_compare[1].imshow(compare_low_pass_render.detach().cpu().numpy())
axes_compare[1].set_title('Final Render:\nLow-Pass Filter')
axes_compare[1].axis('off')

# Plot EWA Filtered Render
axes_compare[2].imshow(compare_ewa_render.detach().cpu().numpy())
axes_compare[2].set_title('Final Render:\nEWA Filter')
axes_compare[2].axis('off')

# Plot Ground Truth
axes_compare[3].imshow(target_tensor.cpu().numpy())
axes_compare[3].set_title('Ground Truth')
axes_compare[3].axis('off')

plt.suptitle("Final Model Output: Anti-Aliasing Filter Comparison", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

print("\n--- Comparison Rendering Complete ---")

